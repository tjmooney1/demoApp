---
title: "topic_modelling"
output: html_document
date: "2024-05-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load and clean data

```{r}
library(tidyverse)
```

```{r cars}
old_beauty <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/hackafun_cosmetic_joined.csv")

# beauty_embeddings <- readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_embeddings.rds")

beauty <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/hackafun_cosmetic_joined.csv") %>%
  dplyr::rename(text_clean = text_copy)


beauty_reduced_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_reduced_embeddings.csv") %>%
  dplyr::select(-1)
  
```

```{r}
#' clean_text
#' @description A function that performs a series of cleaning steps on a text variable. Useful for processing when dealing with qualitative data
#' @param df A tibble or data frame object containing the text variable the user wants to perform cleaning steps upon
#' @param text_var The text variable with the message assigned to the observation that the user wishes to clean
#' @param tolower Whether to convert all text to lower case?
#' @param remove_hashtags Should hashtags be removed?
#' @param remove_mentions Should any user/profile mentions be removed?
#' @param remove_emojis Should emojis be removed?
#' @param remove_punctuation Should punctuation be removed?
#' @param remove_digits Should digits be removed?
#' @param in_parallel Whether to run the function in parallel (TRUE = faster)
#'
#' @return The data object provided, with a cleaned text variable
#'
#' @export
#'
#' @examples
#' if(interactive()){
#' #Performs all cleaning steps in parallel
#'cleaned_data <- clean_text(df = ParseR::sprinklr_export,
#'text_var = Message,
#'in_parallel = TRUE)
#'
#'# If the user wants to perform all cleaning steps but keep capital letters and punctuation 
#'cleaned_data <- clean_text(df = ParseR::sprinklr_export,
#'text_var = Message,
#'tolower = FALSE,
#'remove_punctuation = FALSE,
#'in_parallel = TRUE)
#' }

clean_text_ar <- function(df, text_var = message, tolower = TRUE, remove_hashtags = TRUE, remove_mentions = TRUE, remove_emojis = TRUE, remove_punctuation = TRUE, remove_digits = TRUE, in_parallel = TRUE) {
  text_sym <- rlang::ensym(text_var)
  text_quo <- rlang::enquo(text_var)
  
  # hide regex ----
  # Non-optional regex for websites
  domains <- c(".com", ".ly", ".org", ".net", ".us", ".uk", ".co", ".ch")
  http_regex <- "htt(p|ps)\\S+"
  web_regex <- paste0(
    "[:graph:]*(?=(\\", domains, "/))",
    "|(?<=(\\", domains, "/))[:graph:]*(?![:alnum:])"
  )
  domain_regex <- paste0("(\\", domains, "/)")
  
  # Optional regex for hashtags
  if (remove_hashtags) {
    hashtags_regex <- c("(?<=#)[:graph:]*(?![:graph:])|(?<=#)[:graph:]*$", "#")
  } else {
    hashtags_regex <- NULL
  }
  
  # Optional regex for hashtags
  if (remove_mentions) {
    mentions_regex <- c("(?<=@)[:graph:]*(?![:graph:])|(?<=@)[:graph:]*$", "@")
  } else {
    mentions_regex <- NULL
  }
  
  # Optional regex for emojis
  if (remove_emojis) {
    emojis_regex <- "[^\x01-\x7F]"
  } else {
    emojis_regex <- NULL
  }
  if (remove_punctuation) {
    punctuation_regex <- "[:punct:]"
  } else {
    punctuation_regex <- NULL
  }
  
  if (remove_digits) {
    digits_regex <- "[:digit:]"
  } else {
    digits_regex <- NULL
  }
  
  # Join all regex into one named character vector
  names_regex <- c(
    web_regex,
    domain_regex,
    hashtags_regex,
    mentions_regex,
    digits_regex,
    emojis_regex,
    punctuation_regex,
    http_regex
  )
  all_regex <- character(length(names_regex))
  names(all_regex) <- names_regex
  
  # hide function main body ----
  
  if (tolower) {
    df <- df %>%
      dplyr::mutate(!!text_quo := tolower(!!text_sym))
  }
  
  if (in_parallel) {
    num_cuts <- future::availableCores() - 1
    
    options(future.rng.onMisuse = "ignore")
    message("Beginning parallel sessions")
    future::plan(future::multisession(workers = future::availableCores() - 1))
    
    df <- df %>%
      dplyr::mutate(.document = dplyr::row_number()) %>%
      dplyr::group_split(.document %% num_cuts) %>%
      furrr::future_map_dfr(~ .x %>%
                              dplyr::mutate(!!text_sym := stringr::str_remove_all(!!text_sym, all_regex),
                                            !!text_sym := stringr::str_squish(!!text_sym),
                                            !!text_sym := stringr::str_trim(!!text_sym))) %>%
      dplyr::arrange(.document) %>%
      dplyr::select(-.document)
    
    message("Ending parallel sessions")
    future::plan(future::sequential())
  } else {
    df <- df %>%
      dplyr::mutate(
        !!text_sym := stringr::str_remove_all(!!text_sym, all_regex),
        !!text_sym := stringr::str_squish(!!text_sym)
      ) 
  }
  
  df <- df %>% dplyr::filter(!is.na(!!text_sym))
  return(df)
}

```

This all quick work I did for the demo - should use Tim's dataset not this
```{r}
# mentions_regex <- c("(?<=@)[:graph:]*(?![:graph:])|(?<=@)[:graph:]*$", "@")
mentions_regex <- "@[a-z,A-Z]*?"

beauty_clean <- old_beauty %>%
  dplyr::mutate(text_clean = text) %>%
  clean_text_ar(text_var = text_clean, 
                tolower = FALSE, 
                remove_hashtags = TRUE, 
                remove_mentions = TRUE, 
                remove_emojis = TRUE, 
                remove_punctuation = FALSE, 
                remove_digits = FALSE, 
                in_parallel = TRUE) %>%
  dplyr::mutate(char_length = stringr::str_count(text_clean)) %>%
  dplyr::filter(char_length > 10) %>%
  dplyr::distinct(text_clean, .keep_all = TRUE)

beauty_clean %>% head(100) %>%
  dplyr::select(text, text_copy, text_clean) %>% DT::datatable()


beauty_clean %>% readr::write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/old/cosmetic_joined_rev2.csv")

beauty_clean <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/old/cosmetic_joined_rev2.csv")

reduced_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_joined_reduced_embeddings_ar.csv")

reduced_embeddings2d <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_joined_reduced_embeddings2d_ar.csv")
```

## Quick Topic Modelling for Demo

```{r}
library(BertopicR)

clusterer <- bt_make_clusterer_kmeans(n_clusters = 10L)

model <- bt_compile_model(embedding_model = bt_empty_embedder(),
                          reduction_model = bt_empty_reducer(),
                          clustering_model = clusterer)
bt_fit_model(model, 
             beauty_clean$text_clean, 
             embeddings = reduced_embeddings)

model$get_topic_info() %>% 
  dplyr::select(-Representative_Docs, - Representation) %>%
  DT::datatable()


representation <- bt_representation_openai(fitted_model = model,
                         documents = beauty_clean$text_clean,
                         openai_model = "gpt-3.5-turbo",
                         api_key = Sys.getenv("OPENAI_API_KEY"),
                         nr_repr_docs = 150L,
                         nr_sample = 1000L,
                         chat = TRUE)

topic_summary <- model$get_topic_info() %>%
  dplyr::select(-Representative_Docs, - Representation) %>%
  dplyr::mutate(representation = representation)

topic_lookup <- topic_summary %>% 
  dplyr::select(Topic, representation)

beauty_topics <- beauty_clean %>%
  dplyr::mutate(topic = model$topics_,
                text_with_breaks = sapply(text, insert_line_breaks)) %>%
  dplyr::left_join(topic_lookup, 
                   by = dplyr::join_by("topic" == "Topic")) %>%
  dplyr::rename(topic_title = representation) %>%
  dplyr::select(-c(".document%%num_cuts", "V1", "V2")) %>%
  cbind(reduced_embeddings2d) %>% 
  dplyr::rename(V1 = `0`, V2 = `1`) %>%
  dplyr::mutate(rowid = dplyr::row_number(),
                topic_title = dplyr::case_when(
                  topic == 0 ~ "Hair Care",
                  topic == 1 ~ "Moisturising\Skincare",
                  topic == 2 ~ "Perfumes",
                  topic == 3 ~ "Social Issues",
                  topic == 4 ~ "Colognes",
                  topic == 5 ~ "Skincare for\nAcne/Wrinkles",
                  topic == 6 ~ "Skin Conditions",
                  topic == 7 ~ "Feeling Radiant",
                  topic == 8 ~ "Designers",
                  topic == 9 ~ "Makeup",
                  TRUE ~ NA))

beauty_topics %>%
  readr::write_rds("app/data/cosmetic_data.rds")
```

# Topic Modelling to remove spam
```{r}
library(BertopicR)

clusterer <- bt_make_clusterer_hdbscan(min_cluster_size = 100L, 
                                       min_samples = 50L,
                                       cluster_selection_method = "leaf")

model <- bt_compile_model(embedding_model = bt_empty_embedder(),
                          reduction_model = bt_empty_reducer(),
                          clustering_model = clusterer)
bt_fit_model(model, 
             beauty$text_clean, 
             embeddings = beauty_reduced_embeddings)

model$get_topic_info() %>% 
  dplyr::select(-Representative_Docs, - Representation) %>%
  DT::datatable()


representation <- bt_representation_openai(fitted_model = model,
                         documents = beauty$text_clean,
                         openai_model = "gpt-3.5-turbo",
                         api_key = Sys.getenv("OPENAI_API_KEY"),
                         nr_repr_docs = 150L,
                         nr_sample = 1000L,
                         chat = TRUE)

topic_summary <- model$get_topic_info() %>%
  dplyr::select(-Representative_Docs, - Representation) %>%
  dplyr::mutate(openai_representation = representation)

topic_summary %>% DT::datatable()

beauty %>%
  mutate(topic = model$topics_) %>%
  filter(topic == 50) %>%
  sample_n(100) %>% select(text, text_clean) %>% DT::datatable()
```

The following are clusters that need to be removed:
 c(1, 3, 11, 13, 15, 23, 24, 25, 27, 28, 31, 35, 37, 
 38, 39, 50, 55, 59, 62, 69, 71, 75, 77, 79, 82,  84, 86, 
 87, 92, 94, 95, 96, 97)

The following are interesting clusters:
- 72: Charlotte Tilbury X F1
- Don't remember the numbers but Becky X Maybelline and human Dior

Before actually removing the topics I will perform outlier reduction

```{r}
beauty_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_BAAI_normalised_embeddings.csv") %>%
  dplyr::select(-1)

embedder <- bt_make_embedder_st("BAAI/bge-large-en-v1.5")
outliers <- bt_outliers_embeddings(fitted_model = model,
                                   documents = beauty$text_clean,
                                   topic = model$topics_,
                                   embeddings = beauty_embeddings,
                                   embedding_model = embedder,
                                   threshold = 0.1)

outliers %>% 
  filter(current_topics == -1, new_topics == 49) %>% 
  select(message) %>% DT::datatable()

remove_clusters <- c(1, 3, 11, 13, 15, 23, 24, 25, 27, 28, 31, 35, 37, 
 38, 39, 50, 55, 59, 62, 69, 71, 75, 77, 79, 82,  84, 86, 
 87, 92, 94, 95, 96, 97)

beauty_hdb <- beauty %>%
  mutate(hdb_topic = outliers$new_topics) %>%
  filter(!hdb_topic %in% remove_clusters)


hdb_topic_lookup <-  topic_summary %>% 
  select(Topic, hdb_topic_title = openai_representation)

beauty_hdb %>%
  left_join(hdb_topic_lookup, by = join_by("hdb_topic" == "Topic")) %>% 
  select(hdb_topic, openai_representation) )


beauty_hdb %>%
  readr::write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_hdb_topics.csv")
```

Now let's do some kmeans - first - do I need to rembed?
```{r}
kept_rows <- which(!outliers$new_topics %in% remove_clusters)
beauty_embeddings_subset <- beauty_embeddings[kept_rows,]
beauty_hdb_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_hdb_topics_embeddings.csv") %>%
  dplyr::select(-1)

round(beauty_hdb_embeddings[1:50, 1:10], 5) == round(beauty_embeddings_subset[1:50, 1:10], 5)
```

Conclusion - not exactly the same but close enough that I don't think we need to bother - we would need to re-reduce though. I will do this on colab witht the recalculated embeddings seen as we already have them.

```{r}
beauty_reduced_embeddings_hdb <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_reduced_hdb_embeddings.csv")


kmeans_clusterer <- bt_make_clusterer_kmeans(n_clusters = 10L)
kmeans_clusters <- bt_do_clustering(kmeans_clusterer, beauty_reduced_embeddings_hdb)

cont_tab <- table(kmeans_clusters, beauty_hdb$hdb_topic)
```


Looks like the hdbscan clusters fall fairly neatly into the kmeans ones. Let's save the results.
```{r}
kmeans_model <- bt_compile_model(embedding_model = bt_empty_embedder(),
                          reduction_model = bt_empty_reducer(),
                          clustering_model = kmeans_clusterer)
bt_fit_model(kmeans_model, 
             beauty_hdb$text_clean, 
             embeddings = beauty_reduced_embeddings_hdb)

kmeans_model$get_topic_info() %>% 
  dplyr::select(-Representative_Docs, - Representation) %>%
  DT::datatable()


kmeans_representation <- bt_representation_openai(fitted_model = kmeans_model,
                         documents = beauty_hdb$text_clean,
                         openai_model = "gpt-3.5-turbo",
                         api_key = Sys.getenv("OPENAI_API_KEY"),
                         nr_repr_docs = 150L,
                         nr_sample = 1000L,
                         chat = TRUE)


kmeans_topic_summary <- kmeans_model$get_topic_info() %>%
  dplyr::select(-Representative_Docs, - Representation) %>%
  dplyr::mutate(openai_representation = kmeans_representation)

kmeans_topic_summary %>% DT::datatable()

kmeans_topic_lookup <- kmeans_topic_summary %>%
  select(Topic, kmeans_topic_title = openai_representation)

insert_line_breaks <- function(text, n = 10) {
  words <- strsplit(text, " ")[[1]]
  paste(sapply(seq(1, length(words), n), function(i) {
    paste(words[i:min(i + n - 1, length(words))], collapse = " ")
  }), collapse = "<br>")
}


beauty_topics <- beauty_hdb %>%
  mutate(text_with_breaks = sapply(text, insert_line_breaks),
         kmeans_topic = kmeans_model$topics_) %>%
  left_join(kmeans_topic_lookup, by = join_by("kmeans_topic" == "Topic")) %>%
  left_join(hdb_topic_lookup, by = join_by("hdb_topic" == "Topic")) %>%
  mutate(hdb_topic_title = unlist(hdb_topic_title),
         keans_topic_title = unlist(kmeans_topic_title)) 

beauty_topics %>%
  write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_kmeans_and_hdb_topics.csv")

```

Now I should maybe tokenise

```{r}
install.packages("tidytext")
beauty_sentences <- tidytext::unnest_tokens(
  beauty_topics,
  output = sentences,
  input = text_clean,
  token = "sentences",
  to_lower = FALSE,
  drop = FALSE
  )

beauty_sentences %>%
  write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_sentences.csv")
```

save sentence embeddings as rds
```{r}
cosmetic_sentence_embeddings <- readr::read_csv(
  "~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_sentences_embeddings.csv"
)

cosmetic_sentence_embeddings %>% 
  readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences_embeddings.rds")
```

Fixing topic titles in sentence df (saved as list and so not present when reuploaded)
```{r}
cosmetic_data <- readr::read_rds(here::here("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_df.rds"))

cosmetic_sentences <- readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences.rds")

cosmetic_topic_info <- cosmetic_data %>%
  dplyr::select(kmeans_topic, kmeans_topic_title, hdb_topic, hdb_topic_title, universal_message_id)

cosmetic_sentence_topics <- cosmetic_sentences %>%
  select(-c(kmeans_topic, kmeans_topic_title, hdb_topic, hdb_topic_title)) %>%
  dplyr::left_join(cosmetic_topic_info, by = "universal_message_id")

# quick check
cosmetic_sentence_topics %>%
  distinct(kmeans_topic, kmeans_topic_title)

cosmetic_sentence_topics %>%
  distinct(hdb_topic, hdb_topic_title)

nrow(cosmetic_sentence_topics) == nrow(cosmetic_sentences)

cosmetic_data %>%
  distinct(kmeans_topic, kmeans_topic_title)

cosmetic_data %>%
  distinct(hdb_topic, hdb_topic_title)

cosmetic_sentence_topics %>% readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences.rds")
```
